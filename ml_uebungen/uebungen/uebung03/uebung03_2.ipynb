{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit43d880fb65b94ef98ea0ae8fb79c00a6",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Ãœbung 3: 2. Textklassifikation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import Packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\n",
      "Requirement already satisfied: pandas in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (1.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (1.19.3)\n",
      "Requirement already satisfied: wget in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (3.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (3.3.2)\n",
      "Requirement already satisfied: sklearn in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (0.0)\n",
      "Requirement already satisfied: progressbar in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (2.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (from pandas) (2020.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (from scikit-learn->sklearn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\andreas\\appdata\\roaming\\python\\python38\\site-packages (from scikit-learn->sklearn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy wget matplotlib sklearn progressbar2\n",
    "import os\n",
    "import wget\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "from pathlib import Path\n",
    "import progressbar\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reload_ext version_information\n",
    "#%version_information numpy, pandas"
   ]
  },
  {
   "source": [
    "## a) Download und Entpacken von \"20 Newsgroups\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Folder already downloaded!\n\nFolder already extracted!\n"
     ]
    }
   ],
   "source": [
    "url = 'http://qwone.com/~jason/20Newsgroups/20news-18828.tar.gz'\n",
    "allNewsGroups_tarfile = '20news-18828.tar.gz'\n",
    "allNewsGroups_folder = os.path.join(os.getcwd(), '20news-18828')\n",
    "\n",
    "# Download file\n",
    "if not os.path.isfile(allNewsGroups_tarfile):\n",
    "    print(\"Downloading file...\\n\")\n",
    "\n",
    "    wget.download(url, allNewsGroups_tarfile)\n",
    "    dateDownloaded = !date #Calling Linux\n",
    "    print(\"File downloaded on:\", dateDownloaded)\n",
    "else:\n",
    "      print(\"Folder already downloaded!\\n\")\n",
    "\n",
    "# Extract file\n",
    "if not os.path.isdir(allNewsGroups_folder):\n",
    "    try:\n",
    "        os.mkdir(allNewsGroups_folder)\n",
    "        print(\"Extracting...\")\n",
    "        tar = tarfile.open(allNewsGroups_tarfile, \"r:gz\")\n",
    "        tar.extractall()\n",
    "        print(\"Files extracted\")\n",
    "        tar.close()\n",
    "    except:\n",
    "        print(\"Extraction failed! Something went wrong.\")\n",
    "else:\n",
    "    print(\"Folder already extracted!\")"
   ]
  },
  {
   "source": [
    "## b) Einlesen der 4 Newsgroups"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def strip_header(text):\n",
    "    _before , _blankline , after = text.partition('\\n\\n')\n",
    "    after.replace('\\n',' ')\n",
    "    return after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path: Path = Path(allNewsGroups_folder)  \n",
    "folder_path = \"c:\\\\Users\\Andreas\\\\Documents\\\\GitHub\\\\ML-DL_repo\\\\uebungen\\\\uebung03\\\\20news-18828\\\\\"\n",
    "categories = ['alt.atheism','comp.graphics', 'sci.space', 'talk.religion.misc']\n",
    "filesurvey = []\n",
    "\n",
    "for newsgroup in categories:\n",
    "    for row in os.walk(os.path.join(path, newsgroup)):   # row beinhaltet jeweils einen Ordnerinhalt\n",
    "        for filename in row[2]:  # row[2] ist ein tupel aus Dateinamen\n",
    "            full_path: Path = Path(row[0]) / Path(filename)   # row[0] ist der Ordnerpfad\n",
    "            with open(full_path, 'r', encoding=\"ISO-8859-1\") as file:\n",
    "                data = strip_header(file.read())\n",
    "            filesurvey.append([newsgroup, filename, data]) \n",
    "                    # full_path.stat().st_mtime, full_path.stat().st_size])\n",
    "\n",
    "\n",
    "# raw_data = []\n",
    "# labels = []\n",
    "\n",
    "# for newsgroup in [\"alt.atheism\", \"comp.graphics\", \"sci.space\", \"talk.religion.misc\"]:\n",
    "#     for subdir, _, files in os.walk(os.path.join(newsgroup20_folder, newsgroup)):\n",
    "#         for file in files:\n",
    "#             labels.append(newsgroup)\n",
    "#             with open(os.path.join(subdir, file), 'r', encoding=\"ISO-8859-1\") as f:\n",
    "#                 s = strip_header(f.read())\n",
    "#                 raw_data.append(s)\n",
    "\n",
    "# print(\"Number of vectors:\", len(raw_data))\n",
    "# print(\"Number of labels:\", len(labels))"
   ]
  },
  {
   "source": [
    "## c) String in Worte (Tokens) zerlegen und Merkmalsvektorberechnung"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l = []  # filesurvey[1][2]  [[:],[2]]\n",
    "for i in range(len(filesurvey)):\n",
    "    stripped_data = strip_header(filesurvey[i][2])\n",
    "    l.extend(re.compile(r\"(?u)\\b[a-zA-Z]+\\b\").findall(stripped_data.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of unique tokens: 26\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = []\n",
    "for tokens in l:\n",
    "    unique_tokens.extend(tokens)\n",
    "\n",
    "unique_tokens = list(set(unique_tokens))\n",
    "print(\"Number of unique tokens: {}\".format(len(unique_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_VETORS_PATH = os.path.join(os.getcwd(), 'feature_vectors.npy')\n",
    "TOKENS_PATH = os.path.join(os.getcwd(), 'tokens.npy')\n",
    "\n",
    "def generate_feature_vector(_data, _tokens):\n",
    "    _feature_vectors = np.zeros(shape=(len(_data), len(_tokens)))\n",
    "\n",
    "    with progressbar.ProgressBar(max_value=len(_data)) as bar:\n",
    "        for i, text in enumerate(_data):\n",
    "            for j, token in enumerate(_tokens):\n",
    "                _feature_vectors[i][j] = text.count(token)\n",
    "            bar.update(i)\n",
    "\n",
    "    _feature_vectors = np.asarray(_feature_vectors)\n",
    "    np.save(FEATURE_VETORS_PATH, _feature_vectors)\n",
    "    np.save(TOKENS_PATH, _tokens)\n",
    "\n",
    "    return _feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No saved feature vectors found. Generating new ones..\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'max_value'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-5f2ccaea4620>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mfeature_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFEATURE_VETORS_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0munique_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTOKENS_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\Andreas\\\\Documents\\\\GitHub\\\\ML-DL_repo\\\\uebungen\\\\uebung03\\\\feature_vectors.npy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-5f2ccaea4620>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No saved feature vectors found. Generating new ones..\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mfeature_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_feature_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-65-7b0ca1ecf40c>\u001b[0m in \u001b[0;36mgenerate_feature_vector\u001b[1;34m(_data, _tokens)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0m_feature_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mprogressbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProgressBar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'max_value'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    feature_vectors = np.load(FEATURE_VETORS_PATH)\n",
    "    unique_tokens = np.load(TOKENS_PATH)\n",
    "    print(\"Loading saved feature vectors.\")\n",
    "except IOError:\n",
    "    print(\"No saved feature vectors found. Generating new ones..\")\n",
    "    feature_vectors = generate_feature_vector(l, unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}